<h1 align="center">Notes</h1>

<p align="center"><a href="https://www.pinterestlabs.com/"><img src="Plabs.jpg"></img></a></p>

## Description:
**Learning Reliable Rewards for LLMs via RLHF: Robustness, Adaptivity and [Beyond](https://pages.beamery.com/Pinterest/page/october-pinterest-labs-talk-series-featuring-tuo-zhao-7zjueht0_z)**: Learning from human preferences is central to aligning large language models. This talk presents a three‑step progression that addresses core challenges and points toward the future of RLHF. 3 Models: Bradley–Terry–Luce (BTL), adaptive preference scaling, generative reward models (GRMs).


## Notes:
#### Professor Tuo Zhao | [Georgia Tech](link)
- Generation information from the speaker here

#### Chuck Rosenberg | [Pinterest](link)
- Generation information from the speaker here


## Resources:
- Pinterest is focusing on LLM's:
    - **Bradley–Terry–Luce (BTL)** reward modeling, which treats corrupted or adversarial comparisons as sparse outliers, stabilizing estimation and yielding reliable rewards.
    - **Adaptive preference scaling** within the BTL family—an instance‑wise weighting that captures varying strengths of preference, providing a more faithful treatment of both ambiguous and decisive comparisons. 
    - **Generative Reward Models (GRMs)**: reward models that reason before judging by producing structured rationales and then issuing a preference signal, trained on pairwise data and used to directly optimize policies. 



## Contact:
<!--- You can add in your linkedin, medium, stack overflow, dev.to account, etc. here --->
If you want to contact me you can reach me at <nelson@oakhalo.com>.

Connect with me on <a href="https://www.linkedin.com/in/ayla-nelson/">LinkedIn</a>

Connect with me on <a href="https://github.com/oakHalo">Oakhalo.dev</a>

<!-- 
### TODO stx: 
Future Structure (stx):
backend
frontend
images
screenShots [contains video link]
troubleShooting [contains issues resolved]
-->
